# Upload CSVfile by clicking on file and on the arrow on the left 

# load CSV file into "pandas" data frame
titanic = pandas.read_csv('titanic.csv')

# Upload CSVfile by clicking on file and on the arrow on the left 

# show some of the data
titanic

# Withraw columns
titanic = titanic.drop(["PassengerId", "Ticket", "Cabin", "Fare"], axis = 1)
titanic

# missing values
titanic.isna().sum()

 ou
#Visualisation:

import missingno as msno
msno.matrix(titanic);

# enlever les missing values
titanic = titanic.dropna()

# Compter le nombre de valeurs
titanic["Survived"].value_counts(normalize=True)

#Obtenir une proportion

proportion_less_than_80 = len(df[df['running time'] < 80]) / len(df)
print(proportion_less_than_80)


# décrire un tableau
titanic["Age"].describe()


# Moyenne, écart-type et quantile
print(numpy.mean(titanic["Age"]))
print(numpy.std(titanic["Age"]))
print(numpy.quantile(titanic["Age"], [0.25, 0.5, 0.75]))

# histogramme
import matplotlib.pyplot as plt
plt.hist(titanic["Age"])
plt.show()

# Stylisable
plt.hist(titanic["Age"], bins = 50, edgecolor="black", color = "orange")
plt.title("A view on the passengers' age")
plt.xlabel("Age (years)")
plt.ylabel("Count")
plt.show()

# Boxplot
from plotnine import geom_boxplot,coord_flip
(ggplot(titanic, aes(x = "Sex", y = "Age")) + geom_boxplot() + coord_flip())


# Stylisable

from plotnine import scale_color_manual
(ggplot(titanic, aes(x = "Pclass", y = "Age", colour = "Sex")) + geom_boxplot() + coord_flip() + \
    scale_color_manual(values = ["red", "black"]) + xlab("Ticket class"))

# Regrouper des données par rapport à 2 colonnes
Amsterdam.groupby(["property_type"])[["accommodates"]]

#  Filtrer par une donnée dans le tableau
Amsterdam.query('property_type == "Boat"')

#add a column
df['ratio'] = df['box office'] / df['budget']

# Sort the DataFrame by the 'ratio' column in descending order
df_sorted = df.sort_values(by='ratio', ascending=False)

# Select the top five films
top_five_films = df_sorted.head(5)

# Créer une colonne avec une condition liée à une colonne (ici vrai ou faux)
df['original'] = df['based on'].apply(lambda x: "True" if x == "Original" else "False")

# Créer un intervalle de confiance
import numpy as np
import scipy.stats as st


# create 90% confidence interval
print(st.norm.interval(confidence=0.90, loc=np.mean(X2), scale=st.sem(X2)))


##################################
### P-value et test statistique pour un E(X) = E(Y) a invalider
import numpy as np
import scipy.stats as st

# Generate some example data

strict_cancel_prices = test5  # Example prices for strict cancellation
flexible_cancel_prices = test3  # Example prices for flexible cancellation

# Perform t-test for independent samples
t_statistic, p_value = st.ttest_ind(strict_cancel_prices, flexible_cancel_prices)

# Set the significance level
alpha = 0.05

# Print the results
print(f'T-statistic: {t_statistic}')
print(f'P-value: {p_value}')

# Check for statistical significance
if p_value < alpha:
    print('Reject the null hypothesis: The mean prices are different.')
else:
    print('Fail to reject the null hypothesis: There is no significant difference in mean prices.')


#################################"
# Membre de gauche de l'intervalle de confiance (95%)
var=numpy.std(Test["totalwgt_kg"])**2
moy=numpy.mean(Test["totalwgt_kg"])
D=moy2-1.96*(numpy.sqrt(var2/2647))
D

# Membre de droite de l'intervalle de confiance (95%)
var=numpy.std(Test["totalwgt_kg"])**2
moy=numpy.mean(Test["totalwgt_kg"])
D=moy2+1.96*(numpy.sqrt(var2/2647))
D

#####################################"

### Faire un scatter plot

## first generate bivariate Normal that looks like Figure 6(a).
# mean of the x-component
meanx = 3
# mean of the y-component
meany = 60000
# standard deviation of the x-component
scalex = 1
# standard deviation of the y-component
scaley = 15000
# correlation (between -1 and +1)
rho = +0.3
# compute covariance matrix
cov = [[scalex**2, scalex * scaley * rho],[scalex * scaley * rho, scaley**2]]
# generate samples from bivariate Normal distribution
samples = stats.multivariate_normal(mean = [meanx, meany], cov = cov).rvs(size = 1000)
# convert to pandas DataFrame
pd = pandas.DataFrame(data = samples, columns = ["neuro", "salary"])
pd
## scatter plot of samples
(ggplot(pd, aes(x = "neuro", y = "salary")) + geom_point() + geom_smooth(method='lm', color = "cornflowerblue"))

#####################################################"

# Missing values v2
df.info()

#Scatter plot v2
x = df.culmen_depth_mm
y = df.culmen_length_mm
sns.lmplot(data = df, x = "culmen_depth_mm", y = "culmen_length_mm", ci = None, line_kws=dict(color="red"))
plt.xlabel("Culmen depth (mm)")
plt.ylabel("Culmen length (mm)")
plt.text(18, 57, "correlation = " + str(round(x.corr(y), 2)), fontsize=15);

# Colorer les groupes (ici par espèces)

sns.lmplot(data = df, x = "culmen_depth_mm", y = "culmen_length_mm", ci = None, line_kws=dict(color="blue"))
plt.xlabel("Culmen depth (mm)")
plt.ylabel("Culmen length (mm)")
plt.text(18, 57, "correlation = " + str(round(x.corr(y), 2)), fontsize=15);
sns.scatterplot(data=df, x="culmen_depth_mm", y="culmen_length_mm", hue="species")

# Avoir la plus petite valeur (donne la ligne) (ici pour le rating)
cereals[cereals["rating"]==cereals["rating"].min()]


# Compter les variables catégorielles
cereals[["mfr", "type"]].value_counts()

# Correlation heatmap
#The following plot shows the correlation between the different numerical columns. 
#The correlation is between -1 and +1, and measures the level of association between two variables: 
#if the correlation is close to one, it means that large values of one variable occur when there are large values of the other variable, and vice versa.

plt.figure(figsize=(12, 10))
numeric_cereals = cereals[['calories', 'weight', 'protein', 'fat', 'fiber', 'sodium', 'carbo', 'sugars','potass', 'vitamins', 'rating']]
mask = numpy.tril(numeric_cereals.corr())

sns.heatmap(numeric_cereals.corr(), cmap="coolwarm", annot=True, fmt='.1g', square=True, mask=mask);

#####################################

#Créer une table de contingence (ici entre les noteam et le genre) (pour les variables binaires)
import scipy.stats as stats
table = df.groupby("noteam")["Gender"].value_counts().unstack()

#Pour effectuer un teste d'indépendance a partir de la table
stats.chi2_contingency(table, correction = False)

############################

Algorithme des K-means

from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=3, init='k-means++',random_state=42)
y = kmeans.fit_predict(df)
print(y)

cereals['cluster'] = y

cereals.groupby(['cluster']).mean(numeric_only = True)



#####################################################################"

import numpy # for numerical methods
import scipy.stats as stats # for standard probability distributions
import pandas # for data frames
import matplotlib.pyplot as plt # for plotting
plt.rcParams['figure.figsize'] = 5, 3 # specifying figure sizes
plt.style.use("grayscale") # a theme that applies to all generated plots
import seaborn as sns # for nice plots too

###########################################################################"
# Visualiser dans le temps

from datetime import datetime
challenger.date = pandas.to_datetime(challenger.date)
sns.scatterplot(data = challenger, x = "date", y = "failure");



##############################################################"

#Conduire une régression logistique
#On fait une régression logistique Ce code effectue une régression logistique pour examiner la relation entre les variables explicatives 'téléphone', 'musique', 'coffee daily' et la variable cible 'missed party'.

#Pseudo R-squared : Une mesure de l'adéquation du modèle. Ici, la valeur est de 0.03466, ce qui suggère que le modèle explique environ 3,5 % de la variance dans la variable dépendante

#Log-Likelihood : La valeur de la fonction log-vraisemblance pour le modèle ajusté. Plus la valeur est élevée, meilleur est l'ajustement du modèle.

#Un coef de corrélation négatif montre que croissent inversement

Y = df.dropna()['missed party']
X = sm.add_constant(df.dropna()[['téléphone', 'musique', "coffee daily"]]).astype(float)
mod = sm.Logit(Y, X)
res = mod.fit()
print(res.summary())


# Visualisation de la corrélation entre 2 variables
temperature_grid = numpy.linspace(start = 30, stop = 100, num = 50)
test_set = pandas.DataFrame(data = {"const": numpy.ones(len(temperature_grid)), "temperature": temperature_grid})
test_set["probability of failure"] =  logitreg.predict(sm.add_constant(temperature_grid))
fig, ax = plt.subplots()
sns.lineplot(data = test_set, x = "temperature", y = "probability of failure", color = "gray", ax = ax)
plt.axvline(31, 0, 1, color = "black", linestyle = "--")
sns.scatterplot(data = challenger, x = "temperature", y = "failure", ax = ax);
plt.xlabel("")
plt.ylabel("");


######################################################""
# Régression linéaire


Fait une régression linéaire pour analyser la corrélation avec le coeeficient de correlation par rapport a la santé mentale

Plus le adjusted R squared est élevé et moins il y'a d'erreur possible

Et on a entre crochets l'intervalle de confiance

# import statsmodels
import statsmodels.api as sm
# perform regression of `Age` on everything else
Y = titanic[~titanic.Age.isnull()]['Age']
X = sm.add_constant(titanic[~titanic.Age.isnull()].drop(['Age', 'Survived'], axis=1)).astype(float)
mod = sm.OLS(Y, X)
res = mod.fit()
print(res.summary())


OU

Y = df["quality"]
X = df[["alcohol", "fixed acidity"]]
mod_train = sm.OLS(Y, sm.add_constant(X))
res_train = mod_train.fit()
res_train.summary()


OU

import statsmodels.api as sm
Y = df.dropna()['santé mentale']
X = sm.add_constant(df.dropna()[['musique', 'sport']]).astype(float)
mod = sm.OLS(Y, X)
res = mod.fit()
print(res.summary())

#######################################################""

#Data viz pour savoir quand les gens ont répondu à un sondage avec une courbe

from datetime import datetime
df["timestamp"] = pandas.to_datetime(df["timestamp"], format = '%Y-%m-%d %H:%M:%S.%f')
df["timestamp"].plot();













